#!/bin/bash
# ะขะตััะธัะพะฒะฐะฝะธะต ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััะธ GPU ะดะปั ERNI-KI
# ะะฒัะพั: ะะปัััะพะฝ ะจัะปัั (Tech Lead)

set -e

# ะฆะฒะตัะฐ ะดะปั ะฒัะฒะพะดะฐ
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

# ะคัะฝะบัะธะธ ะปะพะณะธัะพะฒะฐะฝะธั
log() { echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1"; }
success() { echo -e "${GREEN}โ $1${NC}"; }
warning() { echo -e "${YELLOW}โ๏ธ  $1${NC}"; }
error() { echo -e "${RED}โ $1${NC}"; }
info() { echo -e "${CYAN}โน๏ธ  $1${NC}"; }
section() { echo -e "${PURPLE}๐ $1${NC}"; }

# ะัะพะฒะตัะบะฐ ะดะพัััะฟะฝะพััะธ GPU
check_gpu_availability() {
    section "ะัะพะฒะตัะบะฐ ะดะพัััะฟะฝะพััะธ GPU"
    
    if command -v nvidia-smi &> /dev/null; then
        success "nvidia-smi ะดะพัััะฟะตะฝ"
        
        # ะะฝัะพัะผะฐัะธั ะพ GPU
        local gpu_info=$(nvidia-smi --query-gpu=name,driver_version,memory.total,compute_cap --format=csv,noheader,nounits)
        local gpu_name=$(echo "$gpu_info" | cut -d, -f1 | tr -d ' ')
        local driver_version=$(echo "$gpu_info" | cut -d, -f2 | tr -d ' ')
        local memory_total=$(echo "$gpu_info" | cut -d, -f3 | tr -d ' ')
        local compute_cap=$(echo "$gpu_info" | cut -d, -f4 | tr -d ' ')
        
        success "GPU: $gpu_name"
        success "ะัะฐะนะฒะตั: $driver_version"
        success "ะะฐะผััั: ${memory_total} MB"
        success "Compute Capability: $compute_cap"
        
        # ะัะพะฒะตัะบะฐ ัะตะผะฟะตัะฐัััั ะธ ัะฝะตัะณะพะฟะพััะตะฑะปะตะฝะธั
        local temp=$(nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits)
        local power=$(nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits)
        local power_limit=$(nvidia-smi --query-gpu=power.limit --format=csv,noheader,nounits)
        
        success "ะขะตะผะฟะตัะฐัััะฐ: ${temp}ยฐC"
        success "ะญะฝะตัะณะพะฟะพััะตะฑะปะตะฝะธะต: ${power}W / ${power_limit}W"
        
    else
        error "nvidia-smi ะฝะตะดะพัััะฟะตะฝ"
        return 1
    fi
    echo ""
}

# ะัะพะฒะตัะบะฐ GPU ะฒ Docker
check_gpu_in_docker() {
    section "ะัะพะฒะตัะบะฐ GPU ะฒ Docker ะบะพะฝัะตะนะฝะตัะฐั"
    
    # ะัะพะฒะตัะบะฐ Ollama
    log "ะัะพะฒะตัะบะฐ GPU ะฒ Ollama..."
    local ollama_logs=$(docker-compose logs ollama 2>/dev/null | grep -i gpu | tail -3)
    if [[ "$ollama_logs" == *"cuda"* ]]; then
        success "Ollama ะธัะฟะพะปัะทัะตั CUDA"
        echo "$ollama_logs" | while read line; do
            info "  $line"
        done
    else
        warning "Ollama ะผะพะถะตั ะฝะต ะธัะฟะพะปัะทะพะฒะฐัั GPU"
    fi
    
    # ะัะพะฒะตัะบะฐ ะฟัะพัะตััะพะฒ GPU
    log "ะัะพัะตััั, ะธัะฟะพะปัะทัััะธะต GPU:"
    nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv,noheader | while read line; do
        if [[ "$line" == *"ollama"* ]]; then
            success "  $line"
        else
            info "  $line"
        fi
    done
    
    echo ""
}

# ะขะตััะธัะพะฒะฐะฝะธะต ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััะธ ะณะตะฝะตัะฐัะธะธ
test_generation_performance() {
    section "ะขะตััะธัะพะฒะฐะฝะธะต ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััะธ ะณะตะฝะตัะฐัะธะธ ัะตะบััะฐ"
    
    # ะัะพะฒะตัะบะฐ ะดะพัััะฟะฝะพััะธ Ollama API
    if ! curl -sf http://localhost:11434/api/version &> /dev/null; then
        error "Ollama API ะฝะตะดะพัััะฟะตะฝ"
        return 1
    fi
    
    success "Ollama API ะดะพัััะฟะตะฝ"
    
    # ะะพะปััะตะฝะธะต ัะฟะธัะบะฐ ะผะพะดะตะปะตะน
    local models=$(curl -s http://localhost:11434/api/tags | jq -r '.models[].name' 2>/dev/null || echo "")
    if [ -z "$models" ]; then
        warning "ะะพะดะตะปะธ ะฝะต ะฝะฐะนะดะตะฝั"
        return 1
    fi
    
    local test_model=$(echo "$models" | head -1)
    success "ะขะตััะธัะพะฒะฐะฝะธะต ั ะผะพะดะตะปัั: $test_model"
    
    # ะขะตัั 1: ะะพัะพัะบะธะน ะฟัะพะผะฟั
    log "ะขะตัั 1: ะะพัะพัะบะธะน ะฟัะพะผะฟั"
    local short_prompt="ะัะธะฒะตั!"
    local start_time=$(date +%s.%N)
    
    local response1=$(curl -s -X POST http://localhost:11434/api/generate \
        -H "Content-Type: application/json" \
        -d "{\"model\":\"$test_model\",\"prompt\":\"$short_prompt\",\"stream\":false}")
    
    local end_time=$(date +%s.%N)
    local time1=$(echo "scale=3; $end_time - $start_time" | bc)
    
    if [[ "$response1" == *"response"* ]]; then
        success "ะัะตะผั ะณะตะฝะตัะฐัะธะธ (ะบะพัะพัะบะธะน): ${time1}s"
        local tokens1=$(echo "$response1" | jq -r '.eval_count // 0')
        if [ "$tokens1" -gt 0 ]; then
            local tokens_per_sec1=$(echo "scale=1; $tokens1 / $time1" | bc)
            success "ะกะบะพัะพััั: ${tokens_per_sec1} ัะพะบะตะฝะพะฒ/ัะตะบ"
        fi
    else
        error "ะัะธะฑะบะฐ ะณะตะฝะตัะฐัะธะธ ะบะพัะพัะบะพะณะพ ัะตะบััะฐ"
    fi
    
    # ะขะตัั 2: ะะปะธะฝะฝัะน ะฟัะพะผะฟั
    log "ะขะตัั 2: ะะปะธะฝะฝัะน ะฟัะพะผะฟั"
    local long_prompt="ะะฐััะบะฐะถะธ ะฟะพะดัะพะฑะฝะพ ะพ ะฟัะตะธะผััะตััะฒะฐั ะธัะฟะพะปัะทะพะฒะฐะฝะธั GPU ะดะปั ะผะฐัะธะฝะฝะพะณะพ ะพะฑััะตะฝะธั ะธ ะณะตะฝะตัะฐัะธะธ ัะตะบััะฐ. ะะฑัััะฝะธ ัะตัะฝะธัะตัะบะธะต ะดะตัะฐะปะธ."
    local start_time2=$(date +%s.%N)
    
    local response2=$(curl -s -X POST http://localhost:11434/api/generate \
        -H "Content-Type: application/json" \
        -d "{\"model\":\"$test_model\",\"prompt\":\"$long_prompt\",\"stream\":false}")
    
    local end_time2=$(date +%s.%N)
    local time2=$(echo "scale=3; $end_time2 - $start_time2" | bc)
    
    if [[ "$response2" == *"response"* ]]; then
        success "ะัะตะผั ะณะตะฝะตัะฐัะธะธ (ะดะปะธะฝะฝัะน): ${time2}s"
        local tokens2=$(echo "$response2" | jq -r '.eval_count // 0')
        if [ "$tokens2" -gt 0 ]; then
            local tokens_per_sec2=$(echo "scale=1; $tokens2 / $time2" | bc)
            success "ะกะบะพัะพััั: ${tokens_per_sec2} ัะพะบะตะฝะพะฒ/ัะตะบ"
        fi
    else
        error "ะัะธะฑะบะฐ ะณะตะฝะตัะฐัะธะธ ะดะปะธะฝะฝะพะณะพ ัะตะบััะฐ"
    fi
    
    # ะขะตัั 3: ะะฐัะฐะปะปะตะปัะฝัะต ะทะฐะฟัะพัั
    log "ะขะตัั 3: ะะฐัะฐะปะปะตะปัะฝัะต ะทะฐะฟัะพัั (3 ะพะดะฝะพะฒัะตะผะตะฝะฝะพ)"
    local parallel_start=$(date +%s.%N)
    
    for i in {1..3}; do
        {
            local req_start=$(date +%s.%N)
            local req_response=$(curl -s -X POST http://localhost:11434/api/generate \
                -H "Content-Type: application/json" \
                -d "{\"model\":\"$test_model\",\"prompt\":\"ะขะตัั $i: ะะฐะฟะธัะธ ะบะพัะพัะบะธะน ะพัะฒะตั\",\"stream\":false}")
            local req_end=$(date +%s.%N)
            local req_time=$(echo "scale=3; $req_end - $req_start" | bc)
            
            if [[ "$req_response" == *"response"* ]]; then
                echo "ะะฐะฟัะพั $i: ${req_time}s" >> /tmp/gpu_parallel_test.log
            fi
        } &
    done
    
    wait
    local parallel_end=$(date +%s.%N)
    local parallel_total=$(echo "scale=3; $parallel_end - $parallel_start" | bc)
    
    if [ -f /tmp/gpu_parallel_test.log ]; then
        local completed=$(wc -l < /tmp/gpu_parallel_test.log)
        local avg_parallel=$(awk '{sum+=$2; count++} END {print sum/count}' /tmp/gpu_parallel_test.log)
        success "ะะฐะฒะตััะตะฝะพ ะทะฐะฟัะพัะพะฒ: $completed/3"
        success "ะะฑัะตะต ะฒัะตะผั: ${parallel_total}s"
        success "ะกัะตะดะฝะตะต ะฒัะตะผั ะฝะฐ ะทะฐะฟัะพั: ${avg_parallel}s"
        rm -f /tmp/gpu_parallel_test.log
    fi
    
    echo ""
}

# ะะพะฝะธัะพัะธะฝะณ GPU ะฒะพ ะฒัะตะผั ัะฐะฑะพัั
monitor_gpu_usage() {
    section "ะะพะฝะธัะพัะธะฝะณ ะธัะฟะพะปัะทะพะฒะฐะฝะธั GPU"
    
    # ะะฐะฟััะบ ัะพะฝะพะฒะพะณะพ ะผะพะฝะธัะพัะธะฝะณะฐ
    log "ะะฐะฟััะบ ะผะพะฝะธัะพัะธะฝะณะฐ GPU ะฝะฐ 30 ัะตะบัะฝะด..."
    
    # ะกะพะทะดะฐะฝะธะต ัะฐะนะปะฐ ะดะปั ะปะพะณะพะฒ
    local monitor_log="/tmp/gpu_monitor.log"
    > "$monitor_log"
    
    # ะคะพะฝะพะฒัะน ะผะพะฝะธัะพัะธะฝะณ
    {
        for i in {1..30}; do
            local timestamp=$(date +%s)
            local gpu_util=$(nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits)
            local mem_used=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)
            local mem_total=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits)
            local temp=$(nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits)
            local power=$(nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits)
            
            echo "$timestamp,$gpu_util,$mem_used,$mem_total,$temp,$power" >> "$monitor_log"
            sleep 1
        done
    } &
    
    local monitor_pid=$!
    
    # ะัะฟะพะปะฝะตะฝะธะต ัะตััะพะฒะพะน ะฝะฐะณััะทะบะธ
    log "ะัะฟะพะปะฝะตะฝะธะต ัะตััะพะฒะพะน ะฝะฐะณััะทะบะธ..."
    for i in {1..5}; do
        curl -s -X POST http://localhost:11434/api/generate \
            -H "Content-Type: application/json" \
            -d '{"model":"llama3.2:3b","prompt":"ะขะตัั ะฝะฐะณััะทะบะธ GPU","stream":false}' > /dev/null &
    done
    
    wait
    kill $monitor_pid 2>/dev/null || true
    
    # ะะฝะฐะปะธะท ัะตะทัะปััะฐัะพะฒ ะผะพะฝะธัะพัะธะฝะณะฐ
    if [ -f "$monitor_log" ]; then
        log "ะะฝะฐะปะธะท ัะตะทัะปััะฐัะพะฒ ะผะพะฝะธัะพัะธะฝะณะฐ:"
        
        local max_util=$(awk -F, '{if($2>max) max=$2} END {print max}' "$monitor_log")
        local avg_util=$(awk -F, '{sum+=$2; count++} END {print sum/count}' "$monitor_log")
        local max_mem=$(awk -F, '{if($3>max) max=$3} END {print max}' "$monitor_log")
        local max_temp=$(awk -F, '{if($5>max) max=$5} END {print max}' "$monitor_log")
        local max_power=$(awk -F, '{if($6>max) max=$6} END {print max}' "$monitor_log")
        
        success "ะะฐะบัะธะผะฐะปัะฝะฐั ะทะฐะณััะทะบะฐ GPU: ${max_util}%"
        success "ะกัะตะดะฝัั ะทะฐะณััะทะบะฐ GPU: ${avg_util}%"
        success "ะะฐะบัะธะผะฐะปัะฝะพะต ะธัะฟะพะปัะทะพะฒะฐะฝะธะต ะฟะฐะผััะธ: ${max_mem} MB"
        success "ะะฐะบัะธะผะฐะปัะฝะฐั ัะตะผะฟะตัะฐัััะฐ: ${max_temp}ยฐC"
        success "ะะฐะบัะธะผะฐะปัะฝะพะต ัะฝะตัะณะพะฟะพััะตะฑะปะตะฝะธะต: ${max_power}W"
        
        rm -f "$monitor_log"
    fi
    
    echo ""
}

# ะกัะฐะฒะฝะตะฝะธะต ั CPU ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพัััั
compare_cpu_gpu_performance() {
    section "ะกัะฐะฒะฝะตะฝะธะต ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััะธ CPU vs GPU"
    
    info "ะััะพัะธัะตัะบะธะต ะดะฐะฝะฝัะต CPU (ะธะท ะฟัะตะดัะดััะธั ัะตััะพะฒ):"
    info "  ะัะตะผั ะณะตะฝะตัะฐัะธะธ ะฝะฐ CPU: ~2.5s"
    info "  ะะตะถะธะผ ัะฐะฑะพัั: CPU-only"
    
    log "ะขะตะบััะฐั ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััั GPU:"
    local gpu_start=$(date +%s.%N)
    local gpu_response=$(curl -s -X POST http://localhost:11434/api/generate \
        -H "Content-Type: application/json" \
        -d '{"model":"llama3.2:3b","prompt":"ะกัะฐะฒะฝะธัะตะปัะฝัะน ัะตัั ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััะธ","stream":false}')
    local gpu_end=$(date +%s.%N)
    local gpu_time=$(echo "scale=3; $gpu_end - $gpu_start" | bc)
    
    if [[ "$gpu_response" == *"response"* ]]; then
        success "ะัะตะผั ะณะตะฝะตัะฐัะธะธ ะฝะฐ GPU: ${gpu_time}s"
        
        # ะะฐััะตั ััะบะพัะตะฝะธั
        local speedup=$(echo "scale=2; 2.5 / $gpu_time" | bc)
        if (( $(echo "$speedup > 1" | bc -l) )); then
            success "ะฃัะบะพัะตะฝะธะต: ${speedup}x ะฑััััะตะต CPU"
        elif (( $(echo "$speedup < 1" | bc -l) )); then
            warning "GPU ะผะตะดะปะตะฝะฝะตะต CPU ะฒ ${speedup}x ัะฐะท"
        else
            info "ะัะพะธะทะฒะพะดะธัะตะปัะฝะพััั GPU ะธ CPU ัะพะฟะพััะฐะฒะธะผะฐ"
        fi
        
        # ะะฝะฐะปะธะท ัะพะบะตะฝะพะฒ
        local tokens=$(echo "$gpu_response" | jq -r '.eval_count // 0')
        if [ "$tokens" -gt 0 ]; then
            local tokens_per_sec=$(echo "scale=1; $tokens / $gpu_time" | bc)
            success "ะกะบะพัะพััั ะณะตะฝะตัะฐัะธะธ: ${tokens_per_sec} ัะพะบะตะฝะพะฒ/ัะตะบ"
        fi
    else
        error "ะัะธะฑะบะฐ ัะตััะธัะพะฒะฐะฝะธั GPU"
    fi
    
    echo ""
}

# ะะตะฝะตัะฐัะธั ะพััะตัะฐ GPU
generate_gpu_report() {
    section "ะััะตั ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััะธ GPU"
    
    local score=0
    local max_score=5
    local issues=()
    local recommendations=()
    
    # ะัะพะฒะตัะบะฐ ะดะพัััะฟะฝะพััะธ GPU
    if nvidia-smi &> /dev/null; then
        score=$((score + 1))
        success "GPU: ะะพัััะฟะตะฝ ะธ ัะฐะฑะพัะฐะตั"
    else
        issues+=("GPU ะฝะตะดะพัััะฟะตะฝ")
    fi
    
    # ะัะพะฒะตัะบะฐ ะธัะฟะพะปัะทะพะฒะฐะฝะธั GPU ะฒ Ollama
    local ollama_gpu=$(docker-compose logs ollama 2>/dev/null | grep -i cuda | wc -l)
    if [ "$ollama_gpu" -gt 0 ]; then
        score=$((score + 1))
        success "Ollama: ะัะฟะพะปัะทัะตั GPU"
    else
        issues+=("Ollama ะฝะต ะธัะฟะพะปัะทัะตั GPU")
    fi
    
    # ะัะพะฒะตัะบะฐ ะฟะฐะผััะธ GPU
    local gpu_memory=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)
    if [ "$gpu_memory" -gt 1000 ]; then
        score=$((score + 1))
        success "ะะฐะผััั GPU: ะะบัะธะฒะฝะพ ะธัะฟะพะปัะทัะตััั (${gpu_memory} MB)"
    else
        warning "ะะฐะผััั GPU: ะะธะทะบะพะต ะธัะฟะพะปัะทะพะฒะฐะฝะธะต (${gpu_memory} MB)"
        recommendations+=("ะัะพะฒะตัััะต ะฝะฐัััะพะนะบะธ GPU ะฒ Ollama")
    fi
    
    # ะัะพะฒะตัะบะฐ ัะตะผะฟะตัะฐัััั
    local gpu_temp=$(nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits)
    if [ "$gpu_temp" -lt 80 ]; then
        score=$((score + 1))
        success "ะขะตะผะฟะตัะฐัััะฐ GPU: ะะพัะผะฐะปัะฝะฐั (${gpu_temp}ยฐC)"
    else
        warning "ะขะตะผะฟะตัะฐัััะฐ GPU: ะััะพะบะฐั (${gpu_temp}ยฐC)"
        recommendations+=("ะัะพะฒะตัััะต ะพัะปะฐะถะดะตะฝะธะต GPU")
    fi
    
    # ะัะพะฒะตัะบะฐ ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััะธ
    local perf_test=$(curl -s -X POST http://localhost:11434/api/generate \
        -H "Content-Type: application/json" \
        -d '{"model":"llama3.2:3b","prompt":"test","stream":false}' | jq -r '.response' 2>/dev/null)
    
    if [[ "$perf_test" != "null" ]] && [[ -n "$perf_test" ]]; then
        score=$((score + 1))
        success "ะัะพะธะทะฒะพะดะธัะตะปัะฝะพััั: GPU ะณะตะฝะตัะธััะตั ัะตะบัั"
    else
        issues+=("ะัะพะฑะปะตะผั ั ะณะตะฝะตัะฐัะธะตะน ะฝะฐ GPU")
    fi
    
    # ะัะพะณะพะฒะฐั ะพัะตะฝะบะฐ
    local percentage=$((score * 100 / max_score))
    echo ""
    
    if [ "$percentage" -ge 90 ]; then
        success "ะะขะะะะะะฏ ะะฆะะะะ GPU: ${percentage}% - ะัะปะธัะฝะพ"
    elif [ "$percentage" -ge 70 ]; then
        info "ะะขะะะะะะฏ ะะฆะะะะ GPU: ${percentage}% - ะฅะพัะพัะพ"
    elif [ "$percentage" -ge 50 ]; then
        warning "ะะขะะะะะะฏ ะะฆะะะะ GPU: ${percentage}% - ะฃะดะพะฒะปะตัะฒะพัะธัะตะปัะฝะพ"
    else
        error "ะะขะะะะะะฏ ะะฆะะะะ GPU: ${percentage}% - ะขัะตะฑัะตั ะฒะฝะธะผะฐะฝะธั"
    fi
    
    # ะัะพะฑะปะตะผั
    if [ ${#issues[@]} -gt 0 ]; then
        echo ""
        error "ะะฑะฝะฐััะถะตะฝะฝัะต ะฟัะพะฑะปะตะผั:"
        for issue in "${issues[@]}"; do
            echo "  โข $issue"
        done
    fi
    
    # ะะตะบะพะผะตะฝะดะฐัะธะธ
    if [ ${#recommendations[@]} -gt 0 ]; then
        echo ""
        warning "ะะตะบะพะผะตะฝะดะฐัะธะธ:"
        for rec in "${recommendations[@]}"; do
            echo "  โข $rec"
        done
    fi
    
    # ะะฑัะธะต ัะตะบะพะผะตะฝะดะฐัะธะธ
    echo ""
    info "ะะตะบะพะผะตะฝะดะฐัะธะธ ะฟะพ ะพะฟัะธะผะธะทะฐัะธะธ GPU:"
    echo "  โข ะัะฟะพะปัะทัะนัะต ะฑะพะปะตะต ะบััะฟะฝัะต ะผะพะดะตะปะธ ะดะปั ะปัััะตะณะพ ะธัะฟะพะปัะทะพะฒะฐะฝะธั GPU"
    echo "  โข ะะพะฝะธัะพัััะต ัะตะผะฟะตัะฐัััั GPU ะฟัะธ ะฒััะพะบะธั ะฝะฐะณััะทะบะฐั"
    echo "  โข ะะฐััะผะพััะธัะต ะพะฑะฝะพะฒะปะตะฝะธะต ะดัะฐะนะฒะตัะพะฒ CUDA ะดะปั ะปัััะตะน ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััะธ"
    echo "  โข ะะฐัััะพะนัะต ะปะธะผะธัั ะฟะฐะผััะธ GPU ะฒ Docker Compose"
}

# ะัะฝะพะฒะฝะฐั ััะฝะบัะธั
main() {
    echo -e "${PURPLE}"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo "โ                    GPU Performance Test                     โ"
    echo "โ              ะขะตััะธัะพะฒะฐะฝะธะต ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััะธ GPU            โ"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo -e "${NC}"
    
    check_gpu_availability
    check_gpu_in_docker
    test_generation_performance
    monitor_gpu_usage
    compare_cpu_gpu_performance
    generate_gpu_report
    
    echo ""
    echo -e "${GREEN}"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo "โ            ะขะตััะธัะพะฒะฐะฝะธะต GPU ะทะฐะฒะตััะตะฝะพ                       โ"
    echo "โ         ะะตะทัะปััะฐัั ัะพััะฐะฝะตะฝั ะฒ gpu_performance.txt          โ"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo -e "${NC}"
}

# ะะฐะฟััะบ ัะตััะธัะพะฒะฐะฝะธั
main "$@" | tee gpu_performance.txt
