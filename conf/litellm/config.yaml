# LiteLLM Unified Configuration для ERNI-KI
# Единый прокси для всех AI моделей с интеграцией OpenAI API и локальных Ollama моделей

# Модели управляются через Database Models в LiteLLM Admin UI
# Все модели мигрированы из конфигурации в базу данных для полного управления через веб-интерфейс
#
# === vLLM INTEGRATION CONFIGURATION ===
# Для добавления vLLM моделей через Admin UI используйте следующие параметры:
#
# vLLM Model Configuration (добавить через LiteLLM Admin UI):
# - model_name: "vllm/llama-3.1-8b-instruct"
# - litellm_params:
#     model: "meta-llama/Llama-3.1-8B-Instruct"
#     api_base: "http://vllm:8000/v1"
#     api_key: "erni-ki-vllm-secure-key-2024"
#     custom_llm_provider: "openai"
#     max_tokens: 4096
#     temperature: 0.7
#
# Fallback Configuration (настроить через Admin UI):
# - Primary: vllm/llama-3.1-8b-instruct (высокая производительность)
# - Fallback: ollama/llama3.1:8b (совместимость)
#

# Модели управляются через Database Models в LiteLLM Admin UI
# Все модели мигрированы из конфигурации в базу данных для полного управления через веб-интерфейс
#

router_settings:
  num_retries: 3
  timeout: 600
  routing_strategy: "usage-based-routing-v2"
  fallback_models: [] # Fallback модели будут настроены через Database Models
  enable_fallbacks: true # Включить fallback для production
  cooldown_time: 30 # Время ожидания перед повторной попыткой (секунды)
  allowed_fails: 3 # Количество неудачных попыток перед fallback
  # === CONTEXT WINDOW FALLBACK CONFIGURATION ===
  # Автоматическое переключение на модели с большим контекстом при превышении лимита
  context_window_fallbacks:
    # Для моделей с контекстом 8K переключаться на доступные модели
    - model_group: "swiss-ai/apertus-70b-instruct"
      fallback_models:
        - "ollama/gpt-oss:20b"  # 8K context, но более эффективная модель
        - "ollama/gemma3:12b"  # 8K context
        - "ollama/llama3.2:latest"  # 128K context (3.2B параметров)
    - model_group: "apertus-70b-instruct"
      fallback_models:
        - "ollama/gpt-oss:20b"
        - "ollama/gemma3:12b"
        - "ollama/llama3.2:latest"
  # Redis настройки для router временно отключены из-за несовместимости
  # redis_host: "redis"
  # redis_port: 6379
  # redis_password: "ErniKiRedisSecurePassword2024"
  # redis_db: 1 # Используем ту же DB что и для кэширования

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  ui_username: os.environ/UI_USERNAME
  ui_password: os.environ/UI_PASSWORD
  max_budget: 100
  # === DATABASE STORAGE SETTINGS ===
  # Используем Database Models — все модели добавляются через Admin UI/API.
  store_model_in_db: true
  # Включить хранение промптов и ответов в логах трат (для детального логирования)
  store_prompts_in_spend_logs: true
  # Принудительно сохранять все данные запросов
  store_audit_logs_in_db: true
  # Отключить фильтрацию чувствительных данных для отладки
  redact_user_api_key_info: false
  # Дополнительные настройки для сохранения промптов
  disable_spend_logs: false
  disable_error_logs: false
  # === DETAILED LOGGING SETTINGS ===
  # Включить детальное логирование запросов и ответов
  detailed_debug: true
  # Логировать все параметры запросов
  log_raw_request: true
  # Логировать полные ответы
  log_raw_response: true
  # === SECURITY SETTINGS ===
  # Обязательная авторизация для всех /v1/* эндпоинтов (отключено для совместимости с OpenWebUI)
  enforce_user_param: false
  # Разрешить только аутентифицированные запросы
  # Включить аудит логи для безопасности
  enable_audit_logs: true
  # === HEALTH CHECK BYPASS ===
  # Отключить аутентификацию для health check эндпоинтов
  disable_auth_on_health_check: true
  # Отключить логирование health check запросов для уменьшения шума в логах
  disable_health_check_logs: true

# === OPENAI PASSTHROUGH CONFIGURATION ===
# Настройка passthrough для OpenAI Assistant API (обновлено 2025-10-07)
openai_route_config:
  # Включить OpenAI passthrough
  enable_openai_passthrough: true
  # OpenAI API настройки
  openai_api_key: os.environ/OPENAI_API_KEY
  openai_api_base: os.environ/OPENAI_API_BASE
  # Разрешенные эндпоинты для passthrough
  allowed_routes:
    - "/v1/assistants"
    - "/v1/assistants/*"
    - "/v1/threads"
    - "/v1/threads/*"
    - "/v1/threads/*/messages"
    - "/v1/threads/*/messages/*"
    - "/v1/threads/*/runs"
    - "/v1/threads/*/runs/*"
  # Настройки безопасности для passthrough
  require_auth: true
  budget_alerts: false
  fallbacks: false

litellm_settings:
  drop_params: false  # Не удалять параметры для детального логирования
  # set_verbose: true   # Deprecated, используем LITELLM_LOG=DEBUG в env
  num_retries: 3
  request_timeout: 300
  max_parallel_requests: 20  # Ограничение одновременных запросов для снижения пиков памяти
  # === CONTEXT WINDOW MANAGEMENT ===
  # Автоматическое усечение контекста при превышении лимита модели
  enable_context_window_fallback: true
  # Стратегия усечения: "truncate" (обрезать старые сообщения) или "summarize" (суммаризация)
  context_window_fallback_strategy: "truncate"
  # Резервировать 10% контекста для ответа модели
  context_window_safety_margin: 0.1
  # Максимальный размер контекста по умолчанию (32K для большинства моделей)
  max_context_window: 32768
  # === DETAILED LOGGING SETTINGS ===
  # Включить детальное логирование запросов и ответов
  log_raw_request: true
  log_raw_response: true
  # Сохранять промпты в логах
  store_prompts_in_logs: true
  # Alias map для внешних моделей, чтобы сохранять дружественные имена в OpenWebUI.
  model_alias_map:
    apertus-70b-instruct: "swiss-ai/apertus-70b-instruct"
  # === PROMETHEUS METRICS ===
  # Отключено: требует Enterprise лицензию
  # callbacks: ["prometheus"]  # Включить экспорт Prometheus метрик
  # service_callback: ["prometheus_system"]  # Мониторинг системных сервисов (Redis, PostgreSQL)
  # === CACHE CONFIGURATION ===
  # Включено: 2025-10-02 после исправления конфигурации Redis
  cache: true  # Включить кэширование
  # === DETAILED LOG SETTINGS ===
  # Отключить логирование health check запросов для уменьшения шума
  disable_health_check_logs: true
  # Логировать все запросы для детальной отладки
  log_only_errors: false
  # Детальный уровень логирования для отладки
  log_level: "DEBUG"
  # === OLLAMA THINKING TOKENS COMPATIBILITY ===
  # Настройки для правильной обработки thinking tokens
  ignore_unknown_fields: true
  ollama_ignore_thinking: true
  # Включить поддержку thinking tokens в streaming
  enable_thinking_tokens: true
  # Обрабатывать thinking как часть response
  thinking_as_response: true
  # Настройки для Ollama провайдера
  ollama_thinking_mode: "stream"
  # Разрешить неизвестные поля в chunk responses
  allow_unknown_chunk_fields: true
  # === vLLM INTEGRATION SETTINGS ===
  # Настройки для интеграции с vLLM сервером
  vllm_compatibility_mode: true
  # Поддержка vLLM специфичных параметров
  vllm_enable_prefix_caching: true
  # Timeout для vLLM запросов (увеличен для больших моделей)
  vllm_request_timeout: 600
  # Поддержка streaming для vLLM
  vllm_enable_streaming: true
  # === MCP INTEGRATION SETTINGS ===
  # MCP Aliases для упрощения доступа к серверам
  mcp_aliases:
    "deepwiki": "deepwiki_mcp"
  # === KEY GENERATION SETTINGS ===
  # Настройки по умолчанию для генерации ключей
  default_key_generate_params:
    max_budget: 10.0 # Максимальный бюджет по умолчанию $10
    duration: "30d" # Срок действия ключа 30 дней
    metadata:
      created_by: "erni-ki-system"
      environment: "production"
  # Ограничения для генерации ключей
  upperbound_key_generate_params:
    max_budget: 100.0 # Максимальный лимит бюджета $100
    duration: "90d" # Максимальный срок действия 90 дней
    max_parallel_requests: 50 # Максимум параллельных запросов
    tpm_limit: 10000 # Лимит токенов в минуту
    rpm_limit: 100 # Лимит запросов в минуту
  # === PERFORMANCE OPTIMIZATION ===
  # Настройки кэширования для production
  # Включено: 2025-10-02 с использованием in-memory кэширования
  # Redis кэширование имеет баг в LiteLLM v1.77.3 (socket_timeout: 5.0 hardcoded)
  cache_params:
    type: "local" # Использовать in-memory кэширование
    ttl: 1800 # TTL кэша в секундах (30 минут)
    supported_call_types: ["acompletion", "atext_completion", "aembedding", "atranscription"]
  #   socket_connect_timeout: 10 # Timeout подключения
  #   socket_timeout: 10 # Timeout операций
  #   connection_pool_timeout: 5 # Timeout пула подключений
  #   retry_on_timeout: true # Повторить при timeout
  #   health_check_interval: 30 # Интервал проверки здоровья
  # Настройки connection pooling
  connection_pool:
    max_connections: 100 # Максимум подключений к БД
    max_overflow: 20 # Дополнительные подключения при пиковой нагрузке
    pool_timeout: 30 # Таймаут получения подключения
    pool_recycle: 3600 # Время жизни подключения (1 час)

# === OPENAI ASSISTANT API CONFIGURATION ===
# Настройки для интеграции с OpenAI Assistant API (обновлено 2025-10-07)
assistant_settings:
  custom_llm_provider: openai
  litellm_params:
    api_key: os.environ/OPENAI_API_KEY
    api_base: os.environ/OPENAI_API_BASE
  # Настройки для конкретного Assistant
  default_assistant_id: "asst_C8dUl6EKuR41O9sddVVuhTGn"
  # Включить passthrough для Assistant API эндпоинтов
  enable_assistant_passthrough: true
  # Поддержка всех Assistant API операций
  supported_operations:
    - "create_assistant"
    - "get_assistant"
    - "update_assistant"
    - "delete_assistant"
    - "create_thread"
    - "get_thread"
    - "update_thread"
    - "delete_thread"
    - "create_message"
    - "get_message"
    - "list_messages"
    - "create_run"
    - "get_run"
    - "list_runs"
    - "cancel_run"

# === MCP SERVERS CONFIGURATION ===
# Интеграция с внешними MCP серверами (примеры из документации)
mcp_servers:
  # Пример внешнего MCP сервера - DeepWiki
  deepwiki_mcp:
    url: "https://mcp.deepwiki.com/mcp"
    transport: "http"
    description: "DeepWiki MCP Server для поиска информации"
    auth_type: "none"
    spec_version: "2025-03-26"
    mcp_info:
      mcp_server_cost_info:
        default_cost_per_query: 0.001 # $0.001 за запрос
