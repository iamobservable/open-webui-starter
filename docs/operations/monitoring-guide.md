# üìä ERNI-KI Monitoring Guide

Comprehensive guide for monitoring ERNI-KI system with 8 specialized exporters,
standardized healthchecks, and production-ready observability stack.

## üéØ Overview

ERNI-KI monitoring system includes:

- **8 Specialized Exporters** - optimized and standardized (September 19, 2025)
- **Prometheus v3.0.1** - metrics collection and storage (updated October
  24, 2025)
- **27 Alert Rules** - proactive monitoring (18 new system alerts + 9 existing)
- **Grafana v11.6.6** - visualization and dashboards
- **Loki v3.5.5 + Fluent Bit v3.2.0** - centralized logging
- **AlertManager v0.28.0** - notifications and alerting

### üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–æ—è–±—Ä—è 2025

- **Alertmanager queue watchdog** ‚Äî
  `scripts/monitoring/alertmanager-queue-watch.sh` —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫—É
  `alertmanager_cluster_messages_queued` —Å –ø–æ—Ä–æ–≥–∞–º–∏, –ø–∏—à–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –≤
  `logs/alertmanager-queue.log` –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç
  `docker compose restart alertmanager`.
- **Docling shared volume** ‚Äî `scripts/maintenance/docling-shared-cleanup.sh`
  –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ—á–∏—Å—Ç–∫—É `data/docling/shared/uploads` –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
  –ø—Ä–∞–≤.
- **Redis fragmentation** ‚Äî
  `scripts/maintenance/redis-fragmentation-watchdog.sh` –≤—ã–ø–æ–ª–Ω—è–µ—Ç `memory purge`
  –∏ –≤–∫–ª—é—á–∞–µ—Ç `activedefrag` –ø—Ä–∏ ratio >4, –∂—É—Ä–Ω–∞–ª ‚Äî
  `logs/redis-fragmentation-watchdog.log`.
- **TLS & –≤–Ω–µ—à–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏** ‚Äî
  `scripts/infrastructure/security/monitor-certificates.sh` +
  `scripts/infrastructure/monitoring/monitor-rate-limiting.sh` —Å–æ–±–∏—Ä–∞—é—Ç
  –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è proxy/HTTPS –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ nginx/watchtower.
- **LiteLLM Context7 –∫–æ–Ω—Ç—Ä–æ–ª—å** ‚Äî `scripts/monitor-litellm-memory.sh` –ø—É–±–ª–∏–∫—É–µ—Ç
  alert –≤ Slack/Webhook –ø—Ä–∏ —Ä–æ—Å—Ç–µ –ø–∞–º—è—Ç–∏ gateway, –∞
  `scripts/infrastructure/monitoring/test-network-performance.sh` –∏–∑–º–µ—Ä—è–µ—Ç
  latency –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∞ nginx ‚Üí LiteLLM ‚Üí Ollama/PostgreSQL/Redis.
- **Cron & Config Backups** ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã cron-–∑–∞–¥–∞—á –∏ health –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
  —Ñ–∏–∫—Å–∏—Ä—É—é—Ç—Å—è –≤ `docs/archive/config-backup/*.md` (monitoring report, update
  analysis, execution report); –æ–±–Ω–æ–≤–ª—è–π—Ç–µ –∏—Ö –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤ –∏–ª–∏
  —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–π.

## üìã –ê—É–¥–∏—Ç –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ

- –°—á–µ—Ç—á–∏–∫ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –∏–∑
  `docs/archive/reports/documentation-audit-2025-10-24.md` –±–æ–ª—å—à–µ –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç
  –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: 27 –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –∏ Prometheus v3.0.1 –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏
  –æ—Ç—Ä–∞–∂–µ–Ω—ã –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´Prometheus Alerts Configuration¬ª.
- –í—Å–µ 18 –Ω–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏ (Critical,
  Performance, Database, GPU, Nginx) –∏ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞—é—Ç—Å—è —Å–ø–∏—Å–æ–∫–æ–º –∏–Ω–¥–∏–∫–∞—Ü–∏–π, —á—Ç–æ
  –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–ª–µ –∞—É–¥–∏—Ç–∞.
- –†–∞–∑–¥–µ–ª ¬´Monitoring Guide¬ª —Ç–µ–ø–µ—Ä—å —Å–≤—è–∑—ã–≤–∞–µ—Ç—Å—è —Å `prometheus-alerts-guide.md`
  (—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫) –∏ c Runbook‚Äô–∞–º–∏ `operations/operations-handbook.md`,
  `automated-maintenance-guide.md`, —á—Ç–æ–±—ã DevOps –≤–∏–¥–µ–ª –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏
  —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤.

## üìà Exporters Configuration

### üñ•Ô∏è Node Exporter (Port 9101)

**Purpose:** System-level metrics (CPU, memory, disk, network)

```yaml
# Configuration in compose.yml
node-exporter:
  image: prom/node-exporter:v1.8.2
  ports:
    - '9101:9100'
  healthcheck:
    test:
      [
        'CMD-SHELL',
        'wget --no-verbose --tries=1 --spider http://localhost:9100/metrics ||
        exit 1',
      ]
    interval: 30s
    timeout: 10s
    retries: 3
```

**Key Metrics:**

- `node_cpu_seconds_total` - CPU usage by mode
- `node_memory_MemAvailable_bytes` - available memory
- `node_filesystem_avail_bytes` - disk space available
- `node_load1` - 1-minute load average

**Health Check:**

```bash
curl -s http://localhost:9101/metrics | grep node_up
```

### üêò PostgreSQL Exporter (Port 9188 via IPv4 proxy)

**Purpose:** Database performance and health metrics

> ‚ÑπÔ∏è –î–æ—Å—Ç—É–ø –∫ –ë–î –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ Docker secret `postgres_exporter_dsn.txt`
> (–º–æ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ `/etc/postgres_exporter_dsn.txt` –∏ —á–∏—Ç–∞–µ—Ç—Å—è –≤ entrypoint).

```yaml
# Configuration in compose.yml
postgres-exporter:
  image: prometheuscommunity/postgres-exporter:v0.15.0
  entrypoint: ['/entrypoint/postgres-exporter.sh']
  volumes:
    - ./scripts/infrastructure/postgres-exporter-entrypoint.sh:/entrypoint/postgres-exporter.sh:ro
    - ./secrets/postgres_exporter_dsn.txt:/etc/postgres_exporter_dsn.txt:ro
  ports:
    - '127.0.0.1:9188:9188'
  healthcheck:
    test:
      [
        'CMD-SHELL',
        'wget --no-verbose --tries=1 --spider http://localhost:9187/metrics ||
        exit 1',
      ]
```

**Key Metrics:**

- `pg_up` - PostgreSQL availability
- `pg_stat_activity_count` - active connections
- `pg_stat_database_blks_hit` / `pg_stat_database_blks_read` - cache hit ratio
- `pg_locks_count` - database locks

**Health Check:**

```bash
curl -s http://localhost:9188/metrics | grep pg_up
```

### üî¥ Redis Exporter (Port 9121) - üîß Fixed 19.09.2025

**Purpose:** Redis cache performance and health metrics

```yaml
# Configuration in compose.yml (FIXED)
redis-exporter:
  image: oliver006/redis_exporter:v1.62.0
  ports:
    - '127.0.0.1:9121:9121'
  environment:
    - REDIS_EXPORTER_INCL_SYSTEM_METRICS=true
    - REDIS_EXPORTER_LOG_FORMAT=txt
    - REDIS_EXPORTER_DEBUG=true
    - REDIS_ADDR=redis://redis:6379
    - REDIS_PASSWORD_FILE=/run/secrets/redis_exporter_url
  secrets:
    - redis_exporter_url
  healthcheck: {} # monitoring via Prometheus scrape
```

**Status:** ‚úÖ Running | HTTP 200 | Auth works via `REDIS_PASSWORD_FILE`

> `redis_exporter_url` secret —Ç–µ–ø–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç JSON –≤–∏–¥–∞
> `{"redis://redis:6379":"<password>"}` ‚Äî —ç—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç `redis_exporter`
> –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–∞—Ä–æ–ª—å –ø–æ –∞–¥—Ä–µ—Å—É.

**Key Metrics:**

- `redis_up` - Redis availability (now reflects actual status)
- `redis_memory_used_bytes` - memory usage
- `redis_connected_clients` - connected clients
- `redis_keyspace_hits_total` / `redis_keyspace_misses_total` - hit ratio

**Health Check:**

```bash
# HTTP endpoint works (returns metrics)
curl -s http://localhost:9121/metrics | head -5

# TCP healthcheck
timeout 5 sh -c '</dev/tcp/localhost/9121' && echo "Redis Exporter available"

# Direct Redis check (with password)
docker exec erni-ki-redis-1 redis-cli -a ErniKiRedisSecurePassword2024 ping
```

### üéÆ NVIDIA GPU Exporter (Port 9445) - ‚úÖ Improved 19.09.2025

**Purpose:** GPU utilization and performance metrics

```yaml
# Configuration in compose.yml (IMPROVED)
nvidia-exporter:
  image: mindprince/nvidia_gpu_prometheus_exporter:0.1
  ports:
    - '9445:9445'
  healthcheck:
    test: ['CMD-SHELL', "timeout 5 sh -c '</dev/tcp/localhost/9445' || exit 1"] # IMPROVED: TCP check
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 15s
```

**Status:** ‚úÖ Healthy | HTTP 200 | TCP healthcheck (improved from pgrep)

**Key Metrics:**

- `nvidia_gpu_utilization_gpu` - GPU utilization percentage
- `nvidia_gpu_memory_used_bytes` - GPU memory usage
- `nvidia_gpu_temperature_celsius` - GPU temperature
- `nvidia_gpu_power_draw_watts` - power consumption

**Health Check:**

```bash
curl -s http://localhost:9445/metrics | grep nvidia_gpu_utilization
```

### üì¶ Blackbox Exporter (Port 9115)

**Purpose:** External service availability monitoring

```yaml
# Configuration in compose.yml
blackbox-exporter:
  image: prom/blackbox-exporter:v0.25.0
  ports:
    - '9115:9115'
  healthcheck:
    test:
      [
        'CMD-SHELL',
        'wget --no-verbose --tries=1 --spider http://localhost:9115/metrics ||
        exit 1',
      ]
```

**Status:** ‚úÖ Healthy | HTTP 200 | wget healthcheck

**Key Metrics:**

- `probe_success` - probe success status
- `probe_duration_seconds` - probe duration
- `probe_http_status_code` - HTTP response code

**Health Check:**

```bash
curl -s http://localhost:9115/metrics | grep probe_success
```

### üß† Ollama AI Exporter (Port 9778) - ‚úÖ Standardized 19.09.2025

**Purpose:** AI model performance and availability metrics

```yaml
# Configuration in compose.yml (STANDARDIZED)
ollama-exporter:
  build:
    context: ./monitoring
    dockerfile: Dockerfile.ollama-exporter
  ports:
    - '127.0.0.1:9778:9778'
  environment:
    - OLLAMA_URL=http://ollama:11434
    - EXPORTER_PORT=9778
```

**Status:** ‚úÖ Healthy | HTTP 200 | wget healthcheck (standardized from
127.0.0.1)

**Key Metrics:**

- `ollama_models_total` - total number of models
- `ollama_model_size_bytes{model="model_name"}` - model sizes
- `ollama_info{version="x.x.x"}` - Ollama version
- GPU usage for AI workloads

**Health Check:**

```bash
curl -s http://localhost:9778/metrics | grep ollama_models_total
```

### üö™ Nginx Web Exporter (Port 9113) - üîß Fixed 19.09.2025

**Purpose:** Web server performance and traffic metrics

```yaml
# Configuration in compose.yml (FIXED)
nginx-exporter:
  image: nginx/nginx-prometheus-exporter:1.1.0
  ports:
    - '9113:9113'
  command:
    - '--nginx.scrape-uri=http://nginx:80/nginx_status'
    - '--web.listen-address=:9113'
  healthcheck:
    test: ['CMD-SHELL', "timeout 5 sh -c '</dev/tcp/localhost/9113' || exit 1"] # FIXED: TCP check
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 10s
```

**Status:** üîß Running | HTTP 200 | TCP healthcheck (fixed from wget)

**Key Metrics:**

- `nginx_connections_active` - active connections
- `nginx_connections_accepted` - accepted connections
- `nginx_http_requests_total` - total HTTP requests
- `nginx_connections_handled` - handled connections

**Health Check:**

```bash
# HTTP endpoint works
curl -s http://localhost:9113/metrics | grep nginx_connections_active

# TCP healthcheck
timeout 5 sh -c '</dev/tcp/localhost/9113' && echo "Nginx Exporter available"
```

### üìà RAG SLA Exporter (Port 9808)

**Purpose:** RAG (Retrieval-Augmented Generation) performance metrics

```yaml
# Configuration in compose.yml
rag-exporter:
  build: ./monitoring/rag-exporter
  ports:
    - '9808:8000'
  environment:
    - RAG_TEST_URL=http://openwebui:8080
    - RAG_TEST_INTERVAL=30
  healthcheck:
    test:
      [
        'CMD-SHELL',
        'python -c "import requests;
        requests.get(''http://localhost:8000/metrics'')"',
      ]
```

**Status:** ‚úÖ Healthy | HTTP 200 | Python healthcheck

**Key Metrics:**

- `erni_ki_rag_response_latency_seconds` - RAG response latency histogram
- `erni_ki_rag_sources_count` - number of sources in response
- RAG availability and performance SLA tracking

**Health Check:**

```bash
curl -s http://localhost:9808/metrics | grep erni_ki_rag_response_latency
```

## üîß Healthcheck Standardization

### Problems and Solutions (September 19, 2025)

| Exporter            | Problem                        | Solution                             | Status          |
| ------------------- | ------------------------------ | ------------------------------------ | --------------- |
| **Redis Exporter**  | wget unavailable in container  | TCP check `</dev/tcp/localhost/9121` | üîß Fixed        |
| **Nginx Exporter**  | wget unavailable in container  | TCP check `</dev/tcp/localhost/9113` | üîß Fixed        |
| **NVIDIA Exporter** | pgrep process inefficient      | TCP check `</dev/tcp/localhost/9445` | ‚úÖ Improved     |
| **Ollama Exporter** | 127.0.0.1 instead of localhost | wget localhost standardized          | ‚úÖ Standardized |

### Standard Healthcheck Methods

```yaml
# TCP check (for minimal containers without wget/curl)
healthcheck:
  test: ["CMD-SHELL", "timeout 5 sh -c '</dev/tcp/localhost/PORT' || exit 1"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 10s

# HTTP check (for containers with wget)
healthcheck:
  test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:PORT/metrics || exit 1"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 10s

# Custom check (for specialized containers)
healthcheck:
  test: ["CMD-SHELL", "python -c \"import requests; requests.get('http://localhost:PORT/metrics')\""]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 10s
```

## üìä Metrics Verification

### All Exporters Status Check

```bash
# Check all exporters HTTP status
for port in 9101 9188 9121 9445 9115 9778 9113 9808; do
  echo "Port $port: $(curl -s -o /dev/null -w "%{http_code}" http://localhost:$port/metrics)"
done

# Expected output: All ports should return 200
```

### Docker Health Status

```bash
# Check Docker health status
docker ps --format "table {{.Names}}\t{{.Status}}" | grep exporter

# Check specific healthcheck details
docker inspect erni-ki-Redis –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ Grafana --format='{{.State.Health.Status}}'
```

## üö® Troubleshooting Guide

### Common Issues and Solutions

#### 1. Exporter Returns HTTP 200 but Docker Shows No Health Status

**Problem:** Healthcheck configuration uses unavailable tools (wget/curl)
**Solution:** Use TCP check for minimal containers

```bash
# Diagnosis
docker inspect CONTAINER_NAME --format='{{.State.Health}}'

# If returns <nil>, healthcheck is not working
# Fix: Update compose.yml with TCP check
healthcheck:
  test: ["CMD-SHELL", "timeout 5 sh -c '</dev/tcp/localhost/PORT' || exit 1"]
```

#### 2. Redis Exporter Shows redis_up = 0

**Problem:** Authentication issue with Redis **Solution:** Verify Redis
connection string and password

```bash
# Test Redis connection directly
docker exec erni-ki-redis-1 redis-cli -a ErniKiRedisSecurePassword2024 ping

# Check Redis Exporter logs
docker logs erni-ki-Redis –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ Grafana --tail 20
```

#### 3. NVIDIA Exporter Not Showing GPU Metrics

**Problem:** GPU not accessible or NVIDIA runtime not configured **Solution:**
Verify GPU access and runtime

```bash
# Check GPU availability
nvidia-smi

# Check container GPU access
docker exec erni-ki-nvidia-exporter nvidia-smi

# Verify runtime in compose.yml
runtime: nvidia
```

#### 4. Metrics Endpoint Returns 404

**Problem:** Incorrect endpoint path or port configuration **Solution:** Verify
exporter configuration

```bash
# Check container logs
docker logs EXPORTER_CONTAINER --tail 20

# Verify port mapping
docker port EXPORTER_CONTAINER

# Test different endpoints
curl -s http://localhost:PORT/
curl -s http://localhost:PORT/metrics
```

## üö® Prometheus Alerts Configuration

### Overview

ERNI-KI uses **27 active alert rules** for proactive monitoring (added October
24, 2025):

- **18 new system alerts** in `conf/prometheus/alerts.yml`
- **9 existing alerts** in `conf/prometheus/alert_rules.yml` and
  `logging-alerts.yml`

### Alert Groups

#### 1. Critical Alerts (erni-ki-critical-alerts)

**Disk Space Alerts:**

- `DiskSpaceCritical` - Triggers when disk usage >85% (severity: critical,
  ignores `tmpfs`/`vfat` and `/boot/efi`)
- `DiskSpaceWarning` - Triggers when disk usage >75% (severity: warning, ignores
  `tmpfs`/`vfat` and `/boot/efi`)

**Memory Alerts:**

- `MemoryCritical` - Triggers when available memory <5% (severity: critical)
- `MemoryWarning` - Triggers when available memory <15% (severity: warning)

**CPU Alerts:**

- `HighCPUUsage` - Triggers when CPU usage >80% for 5 minutes (severity:
  warning)

**Container Alerts:**

- `ContainerDown` - Triggers when container is down (severity: critical)
- `ContainerRestarting` - Triggers when a non-infrastructure container restarts
  2+ times –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 15 –º–∏–Ω—É—Ç (—Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ
  `container_start_time_seconds`, severity: warning)

**Database Alerts:**

- `PostgreSQLDown` - Triggers when PostgreSQL is unavailable (severity:
  critical)
- `PostgreSQLHighConnections` - Triggers when connections >80 (severity:
  warning)
- `RedisDown` - Triggers when Redis is unavailable (severity: critical)
- `RedisHighMemory` - Triggers when Redis memory >1GB (severity: warning)

**GPU Alerts:**

- `OllamaGPUDown` - Triggers when Ollama GPU is unavailable (severity: critical)
- `OllamaHighVRAM` - Triggers when VRAM usage >80% (severity: warning)

**Nginx Alerts:**

- `NginxDown` - Triggers when Nginx is unavailable (severity: critical)
- `NginxHighErrorRate` - Triggers when 5xx errors >10/min (severity: warning)

#### 2. Performance Alerts (erni-ki-performance-alerts)

- `OpenWebUISlowResponse` - Triggers when response time >5s (severity: warning)
- `SearXNGSlowSearch` - Triggers when search time >3s (severity: warning)
- `DockerStoragePoolAlmostFull` - Triggers when Docker storage >85% (severity:
  warning)

### Alert Configuration

**File:** `conf/prometheus/alerts.yml`

```yaml
groups:
  - name: erni-ki-critical-alerts
    interval: 30s
    rules:
      - alert: DiskSpaceCritical
        expr:
          (node_filesystem_avail_bytes{mountpoint="/"} /
          node_filesystem_size_bytes{mountpoint="/"}) * 100 < 15
        for: 5m
        labels:
          severity: critical
          component: system
        annotations:
          summary: 'Critical: Disk space below 15%'
          description: 'Disk usage is {{ $value }}% on {{ $labels.instance }}'
```

### Viewing Active Alerts

**Prometheus UI:**

```bash
# Open in browser
http://localhost:9091/alerts
```

**API Query:**

```bash
# Get all active alerts
curl -s http://localhost:9091/api/v1/rules | jq '.data.groups[] | select(.name | contains("erni-ki"))'

# Count alerts by severity
curl -s http://localhost:9091/api/v1/rules | jq '.data.groups[].rules[] | select(.labels.severity) | .labels.severity' | sort | uniq -c
```

### Alert Testing

**Trigger test alert:**

```bash
# Test disk space alert (create large file)
dd if=/dev/zero of=/tmp/test-alert.img bs=1G count=10

# Monitor alert status
watch -n 5 'curl -s http://localhost:9091/api/v1/alerts | jq ".data.alerts[] | select(.labels.alertname==\"DiskSpaceCritical\")"'

# Cleanup
rm /tmp/test-alert.img
```

### Monitoring Alertmanager Queue & Disk Alerts

- **Alertmanager queue:** Grafana dashboard `Observability / Alertmanager` ‚Üí
  –ø–∞–Ω–µ–ª—å _Queue Depth_ (–º–µ—Ç—Ä–∏–∫–∞ `alertmanager_cluster_messages_queued`). –ü–æ—Å–ª–µ
  –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª–∞ `ContainerRestarting` –∑–Ω–∞—á–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è <500.
- **Disk utilization sanity:** –ø–∞–Ω–µ–ª—å _Disk Usage by Mount_ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
  –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è (–∏—Å–∫–ª—é—á–∞—è `fstype="vfat"` –∏ `mountpoint="/boot/efi"`). –í
  alert —Å–ø–∏—Å–∫–µ `HighDiskUtilization` —Ç–µ–ø–µ—Ä—å –ø–æ—è–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è `/` –∏ `/data`.
- –î–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:

```bash
curl -s http://localhost:9091/api/v1/query \
  --data-urlencode 'query=alertmanager_cluster_messages_queued'

curl -s http://localhost:9091/api/v1/query \
  --data-urlencode 'query=(1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|vfat",mountpoint!="/boot/efi"} / node_filesystem_size_bytes{fstype!~"tmpfs|vfat",mountpoint!="/boot/efi"})) * 100'
```

### Alert Maintenance

**Reload configuration:**

```bash
# Reload Prometheus configuration
docker compose exec prometheus kill -HUP 1

# Verify alerts loaded
curl -s http://localhost:9091/api/v1/rules | jq '.data.groups[] | .name'
```

**Disable specific alert:**

```yaml
# In conf/prometheus/alerts.yml, comment out the rule
# - alert: AlertName
#   expr: ...
```

### Related Documentation

- [Prometheus Alerts Guide](prometheus-alerts-guide.md) - Detailed alert
  documentation
- [Admin Guide](admin-guide.md) - Alert management procedures

## üìà Performance Optimization

### Metrics Collection Optimization

1. **Scrape Intervals:** Adjust based on metric importance
   - Critical metrics: 15s interval
   - Standard metrics: 30s interval
   - Historical metrics: 60s interval

2. **Retention Policies:** Configure appropriate data retention
   - High-resolution: 7 days
   - Medium-resolution: 30 days
   - Low-resolution: 1 year

3. **Resource Allocation:** Monitor exporter resource usage
   ```bash
   # Check exporter resource usage
   docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}" | grep exporter
   ```

## üéØ Success Criteria

### System Health Indicators

- ‚úÖ **All 8 exporters return HTTP 200** on /metrics endpoint
- ‚úÖ **Docker healthcheck status** shows healthy or running
- ‚úÖ **Prometheus targets** show all exporters as UP
- ‚úÖ **Grafana dashboards** display current metrics
- ‚úÖ **AlertManager** receives and processes alerts

### Performance Targets

- **Response Time:** <2s for all metrics endpoints
- **Availability:** >99.9% uptime for critical exporters
- **Resource Usage:** <5% CPU, <500MB RAM per exporter
- **Data Freshness:** <30s lag for real-time metrics

## üîó Related Documentation

- [Admin Guide](admin-guide.md) - System administration
- [Architecture](../architecture/architecture.md) - System architecture
- [Installation Guide](../getting-started/installation.md) - Setup instructions
- [Troubleshooting](database-troubleshooting.md) - Problem resolution
