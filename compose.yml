# ============================================================================
# 4-УРОВНЕВАЯ СТРАТЕГИЯ ЛОГГИРОВАНИЯ ERNI-KI
# Обновлено: 2025-08-22 - Оптимизированная система согласно аудиту
# ============================================================================

# УРОВЕНЬ 1 - Критически важные сервисы (OpenWebUI, Ollama, PostgreSQL, Nginx)
# Dual logging: fluentd + json-file backup, максимальная надежность
x-critical-logging: &critical-logging
  driver: "fluentd"
  options:
    fluentd-address: "localhost:24224"
    fluentd-async: "true"
    fluentd-retry-wait: "1s"
    fluentd-max-retries: "10"
    tag: "critical.{{.Name}}"
    labels: "service,version,environment,level=critical"
    # Backup в json-file для надежности
    fluentd-buffer-limit: "16777216"

# УРОВЕНЬ 2 - Важные сервисы (SearXNG, Redis, Backrest, Auth, Cloudflared)
# Fluentd с буферизацией, стандартная надежность
x-important-logging: &important-logging
  driver: "fluentd"
  options:
    fluentd-address: "localhost:24224"
    fluentd-async: "true"
    fluentd-retry-wait: "2s"
    fluentd-max-retries: "5"
    tag: "important.{{.Name}}"
    labels: "service,version,environment,level=important"

# УРОВЕНЬ 3 - Вспомогательные сервисы (Docling, EdgeTTS, Tika, MCP)
# JSON-file с компрессией, базовое логгирование
x-auxiliary-logging: &auxiliary-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    compress: "true"
    labels: "service,version,level=auxiliary"

# УРОВЕНЬ 4 - Мониторинговые сервисы (Prometheus, Grafana, Exporters)
# Минимальное логгирование с агрессивной ротацией
x-monitoring-logging: &monitoring-logging
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "2"
    compress: "true"
    labels: "service,version,level=monitoring"

# Обратная совместимость (deprecated, будет удалено в следующих версиях)
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version"

services:
  # Базовые сервисы инфраструктуры
  watchtower:
    # Команда запуска с оптимизированными параметрами (schedule задается через env)
    command: --cleanup --label-enable --http-api-update --http-api-metrics
    env_file: env/watchtower.env
    healthcheck:
      test: ["CMD", "/watchtower", "--health-check"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: containrrr/watchtower:1.7.1 # Закреплена версия для production (последняя стабильная)
    logging: *default-logging
    restart: unless-stopped
    # Добавляем порт для HTTP API
    ports:
      - "8091:8080" # Изменен порт для избежания конфликта
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      # Исключить Watchtower из собственного мониторинга
      - "com.centurylinklabs.watchtower.enable=false"
      # Метка для идентификации в логах
      - "com.centurylinklabs.watchtower.scope=infrastructure"
    # Ограничения ресурсов для предотвращения перегрузки
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.2"
        reservations:
          memory: 128M
          cpus: "0.1"

  # База данных PostgreSQL с векторным расширением (оптимизированная сетевая конфигурация)
  db:
    depends_on:
      watchtower:
        condition: service_healthy
    env_file: env/db.env
    # Добавлено: 2025-10-30 - Docker Secrets для безопасного хранения пароля
    secrets:
      - postgres_password
    environment:
      # Переопределяем POSTGRES_PASSWORD из env файла на использование секрета
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 20s
    image: pgvector/pgvector:pg17
    logging: *critical-logging
    restart: unless-stopped
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      # Кастомная конфигурация PostgreSQL с pg_stat_statements (добавлено: 2025-11-04)
      - ./conf/postgres-enhanced/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    # Используем кастомную конфигурацию
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    # Используем стандартную Docker bridge сеть
    labels:
      # КРИТИЧЕСКИ ВАЖНО: Запретить автообновление базы данных
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-database"

  # Redis кэш и очереди (оптимизированная сетевая конфигурация)
  redis:
    depends_on:
      watchtower:
        condition: service_healthy
    env_file: env/redis.env
    # Используем конфигурационный файл вместо параметров командной строки
    # Обновлено: 2025-10-02 для поддержки active defragmentation
    command: redis-server /usr/local/etc/redis/redis.conf
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 20s
    image: redis:7-alpine # Откат к предыдущей версии (7.2 несовместима с RDB format v12)
    logging: *important-logging
    restart: unless-stopped
    volumes:
      - ./data/redis:/data
      - ./conf/redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    # Используем стандартную Docker bridge сеть
    labels:
      # Разрешить автообновление для Redis
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=cache-services"

  # LiteLLM Context Engineering Gateway (оптимизированная сетевая конфигурация)
  litellm:
    # Используем последнюю версию LiteLLM с поддержкой thinking tokens
    # Обновлено: 2025-10-02 с v1.77.2.rc.1 до v1.77.3-stable для стабильности
    image: ghcr.io/berriai/litellm:v1.77.3-stable
    container_name: erni-ki-litellm

    # Зависимости - запускаем после критически важных сервисов
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy
      watchtower:
        condition: service_healthy

    # Environment файл с конфигурацией
    env_file: env/litellm.env

    # Порт для API (внутренний Docker network)
    ports:
      - "4000:4000" # LiteLLM Proxy API

    # Restart policy для production
    restart: unless-stopped

    # Logging configuration
    logging: *auxiliary-logging

    # Health check для мониторинга состояния (HTTP endpoint проверка)
    # Используем Python urllib вместо curl, так как curl не установлен в контейнере LiteLLM
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python3 -c "import urllib.request; urllib.request.urlopen(''http://localhost:4000/health/liveliness'', timeout=5).read()" || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s # Время на инициализацию

    # Volume mounts для конфигурации
    volumes:
      - ./conf/litellm/config.yaml:/app/config.yaml:ro # Конфигурация моделей
      - ./data/litellm:/app/data # Логи и временные файлы

    # Command с базовыми настройками
    command: ["--config", "/app/config.yaml", "--port", "4000", "--host", "0.0.0.0"]

    # Extra hosts для подключения к Ollama на хосте
    extra_hosts:
      - "host.docker.internal:host-gateway"

    # Resource limits для стабильности (критическое увеличение памяти)
    deploy:
      resources:
        limits:
          memory: 12G # Лимит памяти (увеличено для предотвращения OOM; было 8G)
          cpus: "1.0" # Лимит CPU
        reservations:
          memory: 6G # Минимальная память (увеличена с 4G до 6G для буфера)
          cpus: "0.5" # Минимальный CPU

    # Используем стандартную Docker bridge сеть

    # Labels для мониторинга
    labels:
      - "com.erni-ki.service=litellm"
      - "com.erni-ki.version=main-stable"
      - "com.erni-ki.description=Context Engineering Gateway"
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=ai-services"

  # JWT аутентификация сервис
  auth:
    build:
      context: ./auth
      dockerfile: Dockerfile
    depends_on:
      watchtower:
        condition: service_healthy
    env_file: env/auth.env
    logging: *important-logging
    ports:
      - "9092:9090" # Изменен порт для избежания конфликта
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/app/main", "--health-check"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s
    labels:
      # Разрешить автообновление для Auth сервиса
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=auth-services"

  # Cloudflare туннель для внешнего доступа
  cloudflared:
    command: tunnel --no-autoupdate run
    depends_on:
      watchtower:
        condition: service_healthy
      nginx:
        condition: service_healthy
      openwebui:
        condition: service_healthy
    env_file: env/cloudflared.env
    healthcheck:
      test: ["CMD", "cloudflared", "version"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 30s
    image: cloudflare/cloudflared:2024.10.0 # Стабильная версия (2025.11.0 не существует)
    logging: *important-logging
    restart: unless-stopped
    volumes:
      - ./conf/cloudflare/config:/home/nonroot/.cloudflared
    # Используем стандартную Docker bridge сеть
    labels:
      # Разрешить автообновление для Cloudflared
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=tunnel-services"

  # Сервис извлечения документов
  # Сервис синтеза речи
  edgetts:
    depends_on:
      watchtower:
        condition: service_healthy
    env_file: env/edgetts.env
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python3 -c "import socket; s=socket.socket(); s.settimeout(5); s.connect((\"localhost\", 5050)); s.close()" || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: travisvn/openai-edge-tts:latest # NOTE: Проект использует только latest/canary теги, нет версионных релизов
    logging: *auxiliary-logging
    ports:
      - 5050:5050
    restart: unless-stopped
    labels:
      # Разрешить автообновление для EdgeTTS
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=text-to-speech"

  # Сервис обработки файлов Apache Tika
  tika:
    depends_on:
      watchtower:
        condition: service_healthy
    env_file: env/tika.env
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:9998/tika || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: apache/tika:latest-full # Используем latest (конкретные версии не найдены)
    logging: *auxiliary-logging
    ports:
      - 9998:9998
    restart: unless-stopped
    labels:
      # Разрешить автообновление для Tika
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=document-processing"

  # MCPO сервер для обработки запросов
  mcposerver:
    command: ["--config", "/app/conf/config.json"]
    depends_on:
      watchtower:
        condition: service_healthy
      db:
        condition: service_healthy
    env_file: env/mcposerver.env
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/cmdline"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    image: ghcr.io/open-webui/mcpo:git-91e8f94 # Обновлено: 2025-11-04 с latest на git-91e8f94 (стабильный коммит)
    logging: *auxiliary-logging
    ports:
      - "8000:8000"
    restart: unless-stopped
    volumes:
      - ./conf/mcposerver:/app/conf:ro
      - ./data:/app/data
    # Используем стандартную Docker bridge сеть
    labels:
      # Разрешить автообновление для MCP Server
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=ai-services"

  # Поисковый движок SearXNG (оптимизированная сетевая конфигурация)
  searxng:
    depends_on:
      redis:
        condition: service_healthy
      watchtower:
        condition: service_healthy
    env_file: env/searxng.env
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'wget -q --spider --header="User-Agent: OpenWebUI-HealthCheck/1.0" --header="X-Real-IP: 127.0.0.1" --header="X-Forwarded-For: 127.0.0.1" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.5" http://localhost:8080/ || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: searxng/searxng:latest # Откат: 2025-11-04 с 2025.1.3-a060c0985 на latest (версия 2025.1.3 имеет проблему "unavailable modifier requested")
    logging: *important-logging
    restart: unless-stopped
    volumes:
      - ./conf/searxng/settings.yml:/etc/searxng/settings.yml:ro
      - ./conf/searxng/uwsgi.ini:/etc/searxng/uwsgi.ini:ro
      - ./conf/searxng/limiter.toml:/etc/searxng/limiter.toml:ro
      - ./conf/searxng/favicons.toml:/etc/searxng/favicons.toml:ro
    # Используем стандартную Docker bridge сеть
    labels:
      # Разрешить автообновление для SearXNG
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=search-services"

  # Ollama LLM сервер с GPU ускорением (оптимизированная сетевая конфигурация)
  ollama:
    depends_on:
      watchtower:
        condition: service_healthy
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    env_file: env/ollama.env
    # GPU Memory Limits оптимизированы для RTX 5000 (16GB VRAM)
    # Ollama получает приоритет GPU (управление VRAM через OLLAMA_MAX_VRAM=0.75)
    deploy:
      resources:
        limits:
          cpus: "12.0"
          memory: 16G
        reservations:
          cpus: "8.0"
          memory: 8G
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 10s
    image: ollama/ollama:0.12.9 # Обновлено до последней стабильной версии (2025-11-04) - Fix performance regression, Qwen3-VL, MiniMax-M2 support
    logging: *critical-logging
    ports:
      - 11434:11434
    restart: unless-stopped
    volumes:
      - ./data/ollama:/root/.ollama
    # Используем стандартную Docker bridge сеть
    labels:
      # КРИТИЧЕСКИ ВАЖНО: Запретить автообновление Ollama с GPU
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-ai-gpu"

  # Nginx реверс-прокси (оптимизированная сетевая конфигурация)
  nginx:
    depends_on:
      auth:
        condition: service_healthy
      openwebui:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "/etc/nginx/healthcheck.sh"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 30s
    # Обновлено: 2025-11-04 с 1.28.0 до 1.29.3 (mainline с улучшениями производительности)
    image: nginx:1.29.3
    logging: *critical-logging
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    restart: unless-stopped
    volumes:
      - ./conf/nginx/conf.d/default.conf:/etc/nginx/conf.d/default.conf
      - ./conf/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./conf/nginx/ssl:/etc/nginx/ssl
      - ./conf/nginx/includes:/etc/nginx/includes
      - ./conf/nginx/healthcheck.sh:/etc/nginx/healthcheck.sh:ro
      - ./data/nginx/webroot:/var/www/html
    # Используем стандартную Docker bridge сеть
    labels:
      # КРИТИЧЕСКИ ВАЖНО: Запретить автообновление Nginx (критический прокси-сервер)
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-proxy"

  # OpenWebUI основной интерфейс с GPU ускорением (оптимизированная сетевая конфигурация)
  openwebui:
    depends_on:
      auth:
        condition: service_healthy
      db:
        condition: service_healthy
      litellm:
        condition: service_healthy
      ollama:
        condition: service_healthy
      redis:
        condition: service_healthy
    image: ghcr.io/open-webui/open-webui:v0.6.35 # Обновлено: 2025-11-07 до v0.6.35 (последняя стабильная версия)
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    env_file: env/openwebui.env
    # GPU доступ для OpenWebUI (VRAM управляется через переменные окружения)
    deploy:
      resources:
        limits:
          cpus: "4.0"
          memory: 8G
        reservations:
          cpus: "2.0"
          memory: 4G
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 10s
    logging: *critical-logging
    restart: unless-stopped
    volumes:
      - ./data/openwebui:/app/backend/data
      # Shared volume для передачи файлов в Docling (добавлено 2025-11-07)
    # Используем стандартную Docker bridge сеть
    labels:
      # Разрешить автообновление для OpenWebUI
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=web-interface"

  backrest:
    depends_on:
      - db
      - redis
    env_file: env/backrest.env
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9898/ >/dev/null || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    # Обновлено: 2025-10-02 с v1.4.0 до v1.9.2 (стабильная версия с исправлениями)
    image: garethgeorge/backrest:v1.9.2
    logging: *important-logging
    ports:
      - "9898:9898"
    restart: unless-stopped
    volumes:
      - ./data/backrest:/data
      - ./conf/backrest:/config
      - ./cache/backrest:/cache
      - ./tmp/backrest:/tmp
      - ./data:/backup-sources/data:ro
      - ./conf:/backup-sources/conf:ro
      - ./env:/backup-sources/env:ro
      - ./.config-backup:/backup-sources/.config-backup
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      # Разрешить автообновление для Backrest
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=backup-services"

  # ============================================================================
  # МОНИТОРИНГ И ЛОГИРОВАНИЕ
  # ============================================================================

  # Prometheus - сбор метрик
  prometheus:
    depends_on:
      watchtower:
        condition: service_healthy
      # Убрано 2025-11-07: healthcheck dependency для postgres-exporter, чтобы Prometheus мог запуститься даже если exporter down
      # postgres-exporter:
      #   condition: service_healthy
    image: prom/prometheus:v3.0.0 # Стабильная версия (v3.8.0 не существует)
    container_name: erni-ki-prometheus
    # Исправлено 2025-11-07: явная связь с postgres-exporter для DNS resolution
    links:
      - postgres-exporter
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--storage.tsdb.retention.size=10GB"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--web.enable-lifecycle"
      - "--web.enable-admin-api"
      - "--web.external-url=http://prometheus.erni-ki.local"
    volumes:
      - ./conf/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./conf/prometheus/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
      - ./conf/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro # ДОБАВЛЕНО 2025-10-24: Системные алерты
      - ./conf/prometheus/rules:/etc/prometheus/rules:ro
      - ./conf/prometheus/alerts:/etc/prometheus/alerts:ro # Добавлено: 2025-10-02 для litellm-memory.yml
      - ./data/prometheus:/prometheus
    ports:
      - "9091:9090"
    restart: unless-stopped
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Разрешить автообновление для Prometheus
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Используем стандартную Docker bridge сеть

  # Grafana - dashboard и визуализация
  grafana:
    depends_on:
      watchtower:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    # Обновлено: 2025-10-02 с 10.2.0 до 11.6.6 (последняя стабильная версия 11.x)
    image: grafana/grafana:11.3.0 # Стабильная версия (11.7.0 не существует)
    container_name: erni-ki-grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://grafana.erni-ki.local
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
    volumes:
      - ./data/grafana:/var/lib/grafana
      # Обновленные пути для оптимизированных конфигураций
      - ./conf/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./conf/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3000:3000"
    restart: unless-stopped
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Разрешить автообновление для Grafana
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Используем стандартную Docker bridge сеть

  # Loki - современная система логирования (замена Elasticsearch)
  loki:
    depends_on:
      watchtower:
        condition: service_healthy
    image: grafana/loki:3.0.0 # Стабильная версия (3.6.0 не существует)
    container_name: erni-ki-loki
    logging: *monitoring-logging
    restart: unless-stopped
    ports:
      - "3100:3100"
    volumes:
      - ./conf/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - ./data/loki:/loki
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test:
        ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    labels:
      # Разрешить автообновление для Loki
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=logging-stack"
  # Alertmanager - управление алертами
  alertmanager:
    depends_on:
      watchtower:
        condition: service_healthy
    image: prom/alertmanager:v0.27.0 # Стабильная версия (v0.29.0 не существует)
    container_name: erni-ki-alertmanager
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
      - "--web.external-url=http://alertmanager.erni-ki.local"
      - "--cluster.listen-address=0.0.0.0:9094"
      # Cluster параметры для решения проблемы переполнения очереди
      - "--cluster.gossip-interval=500ms" # Увеличено с 200ms по умолчанию
      - "--cluster.pushpull-interval=120s" # Увеличено с 60s по умолчанию
      - "--cluster.tcp-timeout=15s" # Увеличено для стабильности соединений
      - "--cluster.probe-timeout=1s" # Оптимизировано для быстрой диагностики
      - "--cluster.probe-interval=2s" # Интервал проверки узлов кластера
    volumes:
      - ./conf/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - ./data/alertmanager:/alertmanager
    ports:
      - "9093:9093"
      - "9094:9094"
    restart: unless-stopped
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9093/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Разрешить автообновление для Alertmanager
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Используем стандартную Docker bridge сеть

  # Node Exporter - системные метрики
  node-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
    image: prom/node-exporter:v1.8.2 # Стабильная версия (v1.10.0 не существует)
    container_name: erni-ki-node-exporter
    command:
      - "--path.procfs=/host/proc"
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--path.udev.data=/host/run/udev/data" # Путь к udev данным для diskstats collector
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
      - "--no-collector.systemd" # Отключаем systemd collector (нет доступа к dbus в контейнере)
      # ИСПРАВЛЕНО 2025-10-24: error вместо warn для подавления broken pipe ошибок
      # Broken pipe ошибки - это нормальное поведение когда клиент закрывает соединение
      - "--log.level=error"
      - "--collector.processes"
      - "--web.disable-exporter-metrics" # Отключаем метрики самого экспортера для уменьшения нагрузки
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /run/udev/data:/host/run/udev/data:ro # Монтируем udev данные для diskstats
      - /run/systemd/private:/run/systemd/private:ro
    ports:
      - "9101:9100"
    pid: host
    restart: unless-stopped
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9100/metrics || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      # Разрешить автообновление для Node Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Используем стандартную Docker bridge сеть

  # Postgres Exporter - мониторинг PostgreSQL
  postgres-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
      db:
        condition: service_healthy
    # Возврат к prometheuscommunity образу 2025-11-07: wrouesnel не содержит wget/curl для healthcheck
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: erni-ki-postgres-exporter
    environment:
      - DATA_SOURCE_NAME=postgresql://postgres:aEnbxS4MrXqzurHNGxkcEgCBm@db:5432/openwebui?sslmode=disable
      - PG_EXPORTER_DISABLE_DEFAULT_METRICS=false
      - PG_EXPORTER_DISABLE_SETTINGS_METRICS=true
      - PG_EXPORTER_AUTO_DISCOVER_DATABASES=true
      - PG_EXPORTER_EXCLUDE_DATABASES=template0,template1,postgres
      - PG_EXPORTER_COLLECTOR_STAT_BGWRITER=false
    # Отключено 2025-11-07: конфиг несовместим с v0.17.0
    # volumes:
    #   - ./conf/postgres-exporter/postgres_exporter.yml:/postgres_exporter.yml:ro
    # Публикуем порт 9188 для socat proxy (2025-11-07)
    ports:
      - "9188:9188"
    restart: unless-stopped
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider http://localhost:9187/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    labels:
      # Разрешить автообновление для Postgres Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Используем стандартную Docker bridge сеть

  # Socat Proxy - IPv4 to IPv6 proxy для Postgres Exporter
  # Решение проблемы: Postgres Exporter слушает только на IPv6, а Prometheus подключается по IPv4
  # Используем network_mode: service для доступа к localhost exporter'а (2025-11-07)
  postgres-exporter-proxy:
    depends_on:
      postgres-exporter:
        condition: service_healthy
    image: alpine/socat:latest
    container_name: erni-ki-postgres-exporter-proxy
    network_mode: "service:postgres-exporter"
    command: ["TCP4-LISTEN:9188,fork,reuseaddr", "TCP6:[::1]:9187"]
    restart: unless-stopped
    logging: *monitoring-logging
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # NVIDIA GPU Exporter - мониторинг GPU
  nvidia-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
    image: mindprince/nvidia_gpu_prometheus_exporter:0.1
    container_name: erni-ki-nvidia-exporter
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "9445:9445"
    restart: unless-stopped
    logging: *monitoring-logging
    # Health check отключен: минимальный образ не содержит wget/curl/nc
    # Сервис мониторится через Prometheus scrape успешность
    labels:
      # Разрешить автообновление для NVIDIA Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Используем стандартную Docker bridge сеть

  # Blackbox Exporter - мониторинг доступности
  blackbox-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
    image: prom/blackbox-exporter:v0.25.0 # Стабильная версия (v0.28.0 не существует)
    container_name: erni-ki-blackbox-exporter
    volumes:
      - ./conf/blackbox-exporter/blackbox.yml:/etc/blackbox_exporter/config.yml:ro
    ports:
      - "9115:9115"
    restart: unless-stopped
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9115/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Используем стандартную Docker bridge сеть
    labels:
      # Разрешить автообновление для Blackbox Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # Redis Exporter - мониторинг Redis
  redis-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
      redis:
        condition: service_healthy
    # Обновлено: 2025-10-02 с v1.55.0 до v1.62.0 (стабильная версия без проблем v1.77.0)
    image: oliver006/redis_exporter:v1.62.0 # Откат к стабильной версии (v1.63.0 не существует)
    container_name: erni-ki-redis-exporter
    # Исправлено: 2025-11-07 используем полный URL с паролем (формат redis://:password@host:port)
    environment:
      - REDIS_ADDR=redis://:ErniKiRedisSecurePassword2024@redis:6379
      - REDIS_EXPORTER_INCL_SYSTEM_METRICS=true
      - REDIS_EXPORTER_LOG_FORMAT=txt
      - REDIS_EXPORTER_DEBUG=true
    ports:
      - "9121:9121"
    restart: unless-stopped
    logging: *monitoring-logging
    # Health check отключен: минимальный образ не содержит wget/curl/nc
    # Сервис мониторится через Prometheus scrape успешность
    labels:
      # Разрешить автообновление для Redis Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Используем стандартную Docker bridge сеть

  # Ollama Exporter - мониторинг AI сервисов
  ollama-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
      ollama:
        condition: service_healthy
    build:
      context: ./monitoring
      dockerfile: Dockerfile.ollama-exporter
    container_name: erni-ki-ollama-exporter
    environment:
      - OLLAMA_URL=http://ollama:11434
      - EXPORTER_PORT=9778
      - LOG_LEVEL=info
    ports:
      - "9778:9778"
    restart: unless-stopped
    logging: *monitoring-logging
    # Health check отключен для минимального образа без wget/curl
    # healthcheck:
    #   test:
    #     [
    #       "CMD-SHELL",
    #       "wget --no-verbose --tries=1 --spider http://localhost:9778/metrics || exit 1",
    #     ]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 15s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # Nginx Exporter - мониторинг веб-сервера
  nginx-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
      nginx:
        condition: service_healthy
    # Обновлено: 2025-10-02 с 1.1.0 до 1.4.2 (последняя стабильная версия)
    image: nginx/nginx-prometheus-exporter:1.1.0 # Стабильная версия (1.5.0 не существует)
    container_name: erni-ki-nginx-exporter
    command:
      - "--nginx.scrape-uri=http://nginx:80/nginx_status" # Исправлен порт с 8080 на 80
      - "--web.listen-address=:9113"
    ports:
      - "9113:9113"
    restart: unless-stopped
    logging: *monitoring-logging
    # Health check отключен: минимальный образ не содержит wget/curl/nc
    # Сервис мониторится через Prometheus scrape успешность
    # Метрики доступны на http://localhost:9113/metrics
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # cAdvisor - мониторинг контейнеров
  cadvisor:
    depends_on:
      watchtower:
        condition: service_healthy
    # Обновлено: 2025-10-02 с v0.47.2 до v0.52.1 (последняя доступная версия)
    image: gcr.io/cadvisor/cadvisor:v0.53.0
    container_name: erni-ki-cadvisor
    command:
      - "--housekeeping_interval=10s"
      - "--max_housekeeping_interval=15s"
      - "--docker_only=true"
      - "--disable_metrics=disk,network,tcp,udp,percpu,sched,process"
      - "--store_container_labels=false"
      - "--whitelisted_container_labels=io.kubernetes.container.name,io.kubernetes.pod.name"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
    ports:
      - "8081:8080"
    restart: unless-stopped
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:8080/healthz || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Разрешить автообновление для cAdvisor
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Используем стандартную Docker bridge сеть

  # === ЦЕНТРАЛИЗОВАННОЕ ЛОГИРОВАНИЕ ===

  # Fluent Bit - сборщик и обработчик логов
  fluent-bit:
    depends_on:
      watchtower:
        condition: service_healthy
      loki:
        condition: service_healthy
      # Дополнительная задержка для стабилизации Loki
      prometheus:
        condition: service_healthy
    environment:
      # Уровень логирования
      - FLB_LOG_LEVEL=info
      # HTTP сервер для метрик и health checks
      - FLB_HTTP_SERVER=On
      - FLB_HTTP_LISTEN=0.0.0.0
      - FLB_HTTP_PORT=2020
      # Health check endpoint
      - FLB_HEALTH_CHECK=On
      # Elasticsearch authentication (временно здесь для отладки)
    # Health check для мониторинга состояния Fluent Bit → Loki интеграции
    # Health check отключен: минимальный образ не содержит wget/curl/nc
    # Сервис мониторится через Prometheus scrape успешность и Loki логи
    image: fluent/fluent-bit:3.1.0 # Стабильная версия (3.3.0 не существует)
    container_name: erni-ki-fluent-bit
    logging: *monitoring-logging
    restart: unless-stopped
    volumes:
      # Обновленная конфигурация Fluent Bit для Loki интеграции
      - ./conf/fluent-bit/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro
      - ./conf/fluent-bit/parsers.conf:/fluent-bit/etc/parsers.conf:ro
      - ./conf/fluent-bit/scripts:/fluent-bit/scripts:ro
      # Dedicated volume для логов с оптимизацией для SSD (Фаза 3)
      - erni-ki-logs:/var/log
      - erni-ki-fluent-db:/fluent-bit/db
      # Доступ к Docker socket для сбора логов контейнеров
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - "2020:2020" # HTTP API и health checks
      - "2021:2021" # Prometheus метрики (Фаза 2)
      - "24224:24224" # Fluentd forward protocol
    labels:
      # Разрешить автообновление для Fluent Bit
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=logging-stack"

  # RAG Exporter - метрики SLA RAG (latency, sources)
  rag-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
    build:
      context: ./conf
      dockerfile: Dockerfile.rag-exporter
    container_name: erni-ki-rag-exporter
    environment:
      - PORT=9808
      # URL тестового запроса (по умолчанию health OpenWebUI);
      # можно заменить на реальный RAG endpoint приложения
      - RAG_TEST_URL=http://openwebui:8080/health
      - RAG_TEST_INTERVAL=30
    ports:
      - "9808:9808"
    restart: unless-stopped
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python -c ''import urllib.request,sys; urllib.request.urlopen("http://localhost:9808/metrics", timeout=5); print("ok")'' || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # Webhook Receiver - обработка алертов от Alertmanager
  webhook-receiver:
    depends_on:
      watchtower:
        condition: service_healthy
      alertmanager:
        condition: service_healthy
    build:
      context: ./conf/webhook-receiver
      dockerfile: Dockerfile
    container_name: erni-ki-webhook-receiver
    environment:
      # Порт для webhook endpoints
      - WEBHOOK_PORT=9093
      # Python настройки
      - PYTHONUNBUFFERED=1
      - FLASK_ENV=production
      # Настройки логирования
      - LOG_LEVEL=INFO
    volumes:
      # Логи webhook receiver
      - ./data/webhook-logs:/app/logs
      # Скрипты для обработки алертов
      - ./conf/webhook-receiver/scripts:/app/scripts:ro
    ports:
      - "9095:9093" # Webhook endpoint (избегаем конфликта с Alertmanager 9093)
    restart: unless-stopped
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9093/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.2"
        reservations:
          memory: 128M
          cpus: "0.1"
    labels:
      # Разрешить автообновление для Webhook Receiver
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
# Используем стандартную Docker bridge сеть (docker0)

# ============================================================================
# DEDICATED VOLUMES - Оптимизированные тома для производительности (Фаза 3)
# ============================================================================

volumes:
  # Dedicated volume для логов с оптимизацией для SSD
  erni-ki-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/data/logs-optimized
    labels:
      - "com.erni-ki.volume.type=logs"
      - "com.erni-ki.volume.optimization=ssd"
      - "com.erni-ki.volume.phase=3"

  # Dedicated volume для Fluent Bit database с высокой производительностью
  erni-ki-fluent-db:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/data/fluent-bit-optimized
    labels:
      - "com.erni-ki.volume.type=database"
      - "com.erni-ki.volume.optimization=high-performance"
      - "com.erni-ki.volume.phase=3"

# ============================================================================
# DOCKER SECRETS - Безопасное хранение паролей и API ключей
# ============================================================================
# Добавлено: 2025-10-30 (Фаза 1: Критические исправления безопасности)
# Все чувствительные данные перемещены из env файлов в Docker Secrets
secrets:
  # PostgreSQL пароль (используется: db, openwebui, mcposerver)
  postgres_password:
    file: ./secrets/postgres_password.txt

  # LiteLLM database пароль
  litellm_db_password:
    file: ./secrets/litellm_db_password.txt

  # LiteLLM API ключ (используется: openwebui, litellm)
  litellm_api_key:
    file: ./secrets/litellm_api_key.txt

  # Context7 API ключ (используется: mcposerver)
  context7_api_key:
    file: ./secrets/context7_api_key.txt

  # VLLM API ключ
  vllm_api_key:
    file: ./secrets/vllm_api_key.txt
